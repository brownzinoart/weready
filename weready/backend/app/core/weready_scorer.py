"""
WeReady Score Calculation System
===============================
Calculates the comprehensive WeReady Score based on:
- Code Quality (40%): Hallucination detection, structure, best practices
- Business Model (30%): Revenue model, market validation, unit economics  
- Investment Readiness (30%): Metrics, scalability, fundability

This is the core differentiator that no competitor has.
"""

from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from enum import Enum
import json

class ScoreCategory(Enum):
    CODE_QUALITY = "code_quality"
    BUSINESS_MODEL = "business_model"
    INVESTMENT_READY = "investment_ready"

@dataclass
class ScoreBreakdown:
    category: ScoreCategory
    score: float  # 0-100
    weight: float  # 0-1
    status: str  # "excellent", "good", "needs_work", "critical"
    issues: List[str]
    recommendations: List[str]
    weighted_contribution: float
    detailed_analysis: Dict = field(default_factory=dict)  # Rich analysis data
    insights: List[str] = field(default_factory=list)
    critical_issues: List[str] = field(default_factory=list)
    quick_wins: List[str] = field(default_factory=list)
    long_term_improvements: List[str] = field(default_factory=list)

@dataclass
class WeReadyScore:
    overall_score: int  # 0-100
    breakdown: List[ScoreBreakdown]
    verdict: str  # "ready_to_ship", "needs_work", "critical_issues", "not_ready"
    next_steps: List[str]
    weready_stamp_eligible: bool
    improvement_roadmap: Dict[str, List[str]]

class WeReadyScorer:
    """Calculate comprehensive WeReady Scores for AI-first startups"""
    
    def __init__(self):
        # Scoring weights (must sum to 1.0)
        self.weights = {
            ScoreCategory.CODE_QUALITY: 0.40,      # 40% - Core differentiator
            ScoreCategory.BUSINESS_MODEL: 0.30,    # 30% - Critical for success
            ScoreCategory.INVESTMENT_READY: 0.30   # 30% - VC perspective
        }
        
        # Score thresholds for verdicts
        self.score_thresholds = {
            "ready_to_ship": 85,    # Top 15% - WeReady stamp eligible
            "needs_work": 70,       # Needs improvements but on track
            "critical_issues": 50,  # Major problems to address
            "not_ready": 0          # Significant work required
        }
    
    def calculate_weready_score(
        self, 
        hallucination_result: Dict = None,
        repo_analysis: Dict = None,
        business_data: Dict = None,
        investment_data: Dict = None
    ) -> WeReadyScore:
        """Calculate comprehensive WeReady Score"""
        
        # Calculate individual category scores
        code_score = self._calculate_code_quality_score(hallucination_result, repo_analysis)
        business_score = self._calculate_business_model_score(business_data)
        investment_score = self._calculate_investment_readiness_score(investment_data)
        
        # Calculate weighted overall score
        overall_score = (
            code_score.score * self.weights[ScoreCategory.CODE_QUALITY] +
            business_score.score * self.weights[ScoreCategory.BUSINESS_MODEL] +
            investment_score.score * self.weights[ScoreCategory.INVESTMENT_READY]
        )
        
        # Generate verdict
        verdict = self._determine_verdict(overall_score)
        
        # Check WeReady stamp eligibility
        weready_stamp_eligible = (
            overall_score >= self.score_thresholds["ready_to_ship"] and
            all(breakdown.score >= 70 for breakdown in [code_score, business_score, investment_score])
        )
        
        # Generate next steps and improvement roadmap
        next_steps, improvement_roadmap = self._generate_improvement_plan(
            [code_score, business_score, investment_score], overall_score
        )
        
        return WeReadyScore(
            overall_score=int(overall_score),
            breakdown=[code_score, business_score, investment_score],
            verdict=verdict,
            next_steps=next_steps,
            weready_stamp_eligible=weready_stamp_eligible,
            improvement_roadmap=improvement_roadmap
        )
    
    def _calculate_code_quality_score(self, hallucination_result: Dict = None, repo_analysis: Dict = None) -> ScoreBreakdown:
        """Calculate Code Quality score (40% of overall) with detailed analysis"""
        
        if not hallucination_result:
            return ScoreBreakdown(
                category=ScoreCategory.CODE_QUALITY,
                score=50.0,
                weight=self.weights[ScoreCategory.CODE_QUALITY],
                status="needs_work",
                issues=["No code analysis performed"],
                recommendations=["Analyze your codebase for quality assessment"],
                weighted_contribution=50.0 * self.weights[ScoreCategory.CODE_QUALITY]
            )
        
        base_score = 100.0
        issues = []
        recommendations = []
        
        # Hallucination penalty (most critical)
        hallucination_score = hallucination_result.get('hallucination_score', 0)
        if hallucination_score > 0:
            penalty = min(hallucination_score * 100, 60)  # Max 60 point penalty
            base_score -= penalty
            fake_packages = len(hallucination_result.get('hallucinated_packages', []))
            issues.append(f"{fake_packages} hallucinated packages detected")
            recommendations.append(f"Remove or replace {fake_packages} fake packages immediately")
        
        # AI likelihood factor
        ai_likelihood = hallucination_result.get('ai_likelihood', 0)
        if ai_likelihood > 0.7:
            base_score -= 15  # High AI generation penalty
            issues.append(f"High AI-generation likelihood ({ai_likelihood:.0%})")
            recommendations.append("Add comprehensive testing for AI-generated code")
        elif ai_likelihood > 0.4:
            base_score -= 5   # Moderate AI generation penalty
            issues.append(f"Moderate AI-generation detected ({ai_likelihood:.0%})")
            recommendations.append("Review and validate AI-generated code sections")
        
        # Repository structure bonus (if available)
        if repo_analysis:
            files_analyzed = repo_analysis.get('files_analyzed', 0)
            if files_analyzed > 10:
                base_score += 5  # Well-structured project bonus
            elif files_analyzed < 3:
                base_score -= 10
                issues.append("Very small codebase - limited analysis possible")
                recommendations.append("Expand codebase or provide more comprehensive code samples")
        
        # Confidence factor
        confidence = hallucination_result.get('confidence', 0.8)
        if confidence < 0.7:
            base_score -= 5
            issues.append("Low analysis confidence due to parsing issues")
        
        # Ensure score bounds
        final_score = max(0, min(100, base_score))
        
        # Determine status
        if final_score >= 90:
            status = "excellent"
        elif final_score >= 75:
            status = "good"
        elif final_score >= 50:
            status = "needs_work"  
        else:
            status = "critical"
        
        # Default recommendations for good scores
        if final_score >= 80 and not recommendations:
            recommendations.append("Code quality looks good! Ready for business model validation.")
        
        # Generate detailed analysis
        detailed_analysis = self._generate_code_quality_analysis(hallucination_result, repo_analysis, final_score)
        insights = self._generate_code_insights(hallucination_result, final_score)
        critical_issues, quick_wins, long_term = self._categorize_code_improvements(issues, recommendations, final_score)
        
        return ScoreBreakdown(
            category=ScoreCategory.CODE_QUALITY,
            score=final_score,
            weight=self.weights[ScoreCategory.CODE_QUALITY],
            status=status,
            issues=issues,
            recommendations=recommendations,
            weighted_contribution=final_score * self.weights[ScoreCategory.CODE_QUALITY],
            detailed_analysis=detailed_analysis,
            insights=insights,
            critical_issues=critical_issues,
            quick_wins=quick_wins,
            long_term_improvements=long_term
        )
    
    def _calculate_business_model_score(self, business_data: Dict = None) -> ScoreBreakdown:
        """Calculate Business Model score (30% of overall) with detailed analysis"""
        
        # Rich scoring based on business model fundamentals
        if not business_data:
            base_score = 60.0  # Neutral baseline for startups
            issues = []
            recommendations = [
                "Define your revenue model and pricing strategy",
                "Validate market demand with potential customers", 
                "Calculate unit economics including AI API costs",
                "Identify your target customer segments"
            ]
            status = "needs_work"
        else:
            # TODO: Implement when business_data is available
            base_score = 70.0
            issues = []
            recommendations = ["Continue developing business model"]
            status = "good"
        
        # Generate detailed analysis
        detailed_analysis = self._generate_business_model_analysis(business_data, base_score)
        insights = self._generate_business_insights(base_score)
        critical_issues, quick_wins, long_term = self._categorize_business_improvements(issues, recommendations, base_score)
        
        return ScoreBreakdown(
            category=ScoreCategory.BUSINESS_MODEL,
            score=base_score,
            weight=self.weights[ScoreCategory.BUSINESS_MODEL],
            status=status,
            issues=issues,
            recommendations=recommendations,
            weighted_contribution=base_score * self.weights[ScoreCategory.BUSINESS_MODEL],
            detailed_analysis=detailed_analysis,
            insights=insights,
            critical_issues=critical_issues,
            quick_wins=quick_wins,
            long_term_improvements=long_term
        )
    
    def _calculate_investment_readiness_score(self, investment_data: Dict = None) -> ScoreBreakdown:
        """Calculate Investment Readiness score (30% of overall) with detailed analysis"""
        
        # Rich scoring based on investment readiness factors
        if not investment_data:
            base_score = 55.0  # Lower baseline - investment readiness is challenging
            issues = []
            recommendations = [
                "Establish key metrics tracking (ARR, CAC, LTV)",
                "Achieve product-market fit with early customers",
                "Build a scalable go-to-market strategy",
                "Prepare financial projections and unit economics",
                "Document your competitive advantages and moat"
            ]
            status = "needs_work"
        else:
            # TODO: Implement when investment_data is available
            base_score = 65.0
            issues = []
            recommendations = ["Work on investment readiness metrics"]
            status = "needs_work"
        
        # Generate detailed analysis
        detailed_analysis = self._generate_investment_analysis(investment_data, base_score)
        insights = self._generate_investment_insights(base_score)
        critical_issues, quick_wins, long_term = self._categorize_investment_improvements(issues, recommendations, base_score)
        
        return ScoreBreakdown(
            category=ScoreCategory.INVESTMENT_READY,
            score=base_score,
            weight=self.weights[ScoreCategory.INVESTMENT_READY],
            status=status,
            issues=issues,
            recommendations=recommendations,
            weighted_contribution=base_score * self.weights[ScoreCategory.INVESTMENT_READY],
            detailed_analysis=detailed_analysis,
            insights=insights,
            critical_issues=critical_issues,
            quick_wins=quick_wins,
            long_term_improvements=long_term
        )
    
    def _determine_verdict(self, overall_score: float) -> str:
        """Determine verdict based on overall score"""
        
        if overall_score >= self.score_thresholds["ready_to_ship"]:
            return "ready_to_ship"
        elif overall_score >= self.score_thresholds["needs_work"]:
            return "needs_work"
        elif overall_score >= self.score_thresholds["critical_issues"]:
            return "critical_issues"
        else:
            return "not_ready"
    
    def _generate_improvement_plan(
        self, 
        breakdowns: List[ScoreBreakdown], 
        overall_score: float
    ) -> tuple[List[str], Dict[str, List[str]]]:
        """Generate next steps and improvement roadmap (legacy format for compatibility)"""
        
        next_steps = []
        improvement_roadmap = {
            "immediate": [],      # 0-2 weeks  
            "short_term": [],     # 2-8 weeks
            "long_term": []       # 2+ months
        }
        
        # Sort categories by score (worst first)
        sorted_breakdowns = sorted(breakdowns, key=lambda x: x.score)
        
        # Generate immediate next steps (focus on worst category)
        worst_category = sorted_breakdowns[0]
        if worst_category.score < 70:
            next_steps.append(f"ğŸš¨ Priority: Fix {worst_category.category.value.replace('_', ' ')}")
            improvement_roadmap["immediate"].extend(worst_category.recommendations[:2])
        
        # Add category-specific improvements
        for breakdown in sorted_breakdowns:
            category_name = breakdown.category.value.replace('_', ' ').title()
            
            if breakdown.score < 50:
                improvement_roadmap["immediate"].append(f"Address critical {category_name.lower()} issues")
            elif breakdown.score < 75:
                improvement_roadmap["short_term"].extend(breakdown.recommendations[:1])
            else:
                improvement_roadmap["long_term"].extend(breakdown.recommendations[:1])
        
        # Overall score-based next steps
        if overall_score >= 85:
            next_steps.append("ğŸ‰ You're WeReady! Consider applying for the WeReady Stamp")
            improvement_roadmap["short_term"].append("Document your success story")
        elif overall_score >= 70:
            next_steps.append("ğŸš€ Close to WeReady! Focus on your lowest-scoring areas")
        elif overall_score >= 50:
            next_steps.append("âš ï¸ Significant improvements needed before launch")
        else:
            next_steps.append("ğŸ”§ Major work required. Start with code quality basics")
        
        # Remove duplicates and limit recommendations
        for key in improvement_roadmap:
            improvement_roadmap[key] = list(dict.fromkeys(improvement_roadmap[key]))[:3]
        
        return next_steps[:3], improvement_roadmap  # Limit to top 3 next steps
    
    def generate_intelligent_roadmap(
        self, 
        brain_recommendations: List,
        breakdowns: List[ScoreBreakdown], 
        overall_score: float,
        key_risks: List[str] = None,
        competitive_moats: List[str] = None
    ) -> Dict[str, List[Dict[str, Any]]]:
        """Generate intelligent improvement roadmap using brain recommendations"""
        
        roadmap = {
            "immediate": [],      # 0-2 weeks  
            "short_term": [],     # 2-8 weeks
            "long_term": []       # 2+ months
        }
        
        # Map timeline values to roadmap keys
        timeline_mapping = {
            "immediate": "immediate",
            "1-2 weeks": "immediate", 
            "2-4 weeks": "short_term",
            "1-3 months": "short_term",
            "3+ months": "long_term",
            "ongoing": "long_term"
        }
        
        # Process brain recommendations
        for rec in brain_recommendations:
            # Determine timeline category
            timeline_key = timeline_mapping.get(rec.get("timeline", "short_term"), "short_term")
            
            # Create rich roadmap item
            roadmap_item = {
                "action": rec.get("specific_action", rec.get("recommendation", "Take action")),
                "title": rec.get("recommendation", "Improvement needed"),
                "why": self._generate_why_explanation(rec),
                "how": self._generate_how_steps(rec),
                "success_rate": f"{rec.get('similar_success_cases', 5)} companies succeeded with this approach",
                "confidence": rec.get("confidence", 0.7),
                "confidence_label": self._get_confidence_label(rec.get("confidence", 0.7)),
                "evidence_source": rec.get("evidence_source_name", "Industry research"),
                "priority": rec.get("priority", "medium"),
                "market_context": rec.get("market_context", "Standard practice for startups")
            }
            
            roadmap[timeline_key].append(roadmap_item)
        
        # Add category-specific items for low scores
        for breakdown in sorted(breakdowns, key=lambda x: x.score):
            if breakdown.score < 50:
                critical_item = self._generate_critical_roadmap_item(breakdown, "immediate")
                roadmap["immediate"].append(critical_item)
            elif breakdown.score < 70:
                improvement_item = self._generate_improvement_roadmap_item(breakdown, "short_term")
                roadmap["short_term"].append(improvement_item)
        
        # Add overall score-based strategic items
        strategic_items = self._generate_strategic_roadmap_items(overall_score, key_risks, competitive_moats)
        for item in strategic_items:
            timeline_key = item.get("timeline", "long_term")
            roadmap[timeline_key].append(item)
        
        # Sort by priority and confidence, limit to top items
        for timeline_key in roadmap:
            roadmap[timeline_key] = self._prioritize_roadmap_items(roadmap[timeline_key])[:4]
        
        return roadmap
    
    def _generate_why_explanation(self, rec: Dict[str, Any]) -> str:
        """Generate compelling 'why this matters' explanation"""
        confidence = rec.get("confidence", 0.7)
        success_cases = rec.get("similar_success_cases", 5)
        
        base_explanation = rec.get("market_context", "Important for startup success")
        
        if confidence > 0.9:
            return f"Critical: {base_explanation}. {success_cases} similar startups saw immediate results."
        elif confidence > 0.8:
            return f"High impact: {base_explanation}. Proven by {success_cases} successful cases."
        else:
            return f"Recommended: {base_explanation}. {success_cases} cases show positive outcomes."
    
    def _generate_how_steps(self, rec: Dict[str, Any]) -> List[str]:
        """Generate specific implementation steps"""
        action = rec.get("specific_action", "")
        priority = rec.get("priority", "medium")
        
        if "hallucination" in action.lower():
            return [
                "1. Run package validator on all imports",
                "2. Check npm/pypi registries for package existence", 
                "3. Replace fake packages with real alternatives",
                "4. Set up automated import validation"
            ]
        elif "code review" in action.lower():
            return [
                "1. Set up PR review requirements in GitHub",
                "2. Install automated testing pipeline",
                "3. Create code review checklist",
                "4. Train team on review best practices"
            ]
        elif "product-market fit" in action.lower() or "pmf" in action.lower():
            return [
                "1. Survey 40+ users with Sean Ellis PMF test",
                "2. Aim for 40% 'very disappointed' threshold",
                "3. Analyze feedback for improvement areas",
                "4. Iterate product based on insights"
            ]
        elif "growth" in action.lower() or "revenue" in action.lower():
            return [
                "1. Track monthly revenue growth rate",
                "2. Identify your best growth channel",
                "3. Double down on working strategies",
                "4. Target 15% monthly growth minimum"
            ]
        else:
            return [
                "1. Assess current state and gaps",
                "2. Create implementation plan",
                "3. Execute systematically",
                "4. Measure and iterate"
            ]
    
    def _get_confidence_label(self, confidence: float) -> str:
        """Convert confidence score to readable label"""
        if confidence > 0.9:
            return "Very High"
        elif confidence > 0.8:
            return "High"
        elif confidence > 0.6:
            return "Medium"
        else:
            return "Low"
    
    def _generate_critical_roadmap_item(self, breakdown: 'ScoreBreakdown', timeline: str) -> Dict[str, Any]:
        """Generate critical roadmap item for low-scoring categories"""
        category_name = breakdown.category.value.replace('_', ' ').title()
        
        return {
            "action": f"Address critical {category_name.lower()} issues immediately",
            "title": f"Fix {category_name} Score ({breakdown.score}/100)",
            "why": f"Score below 50 indicates critical issues that prevent funding readiness. Investors will reject based on this alone.",
            "how": [
                f"1. Review {category_name.lower()} analysis details",
                "2. Focus on highest-impact improvements first",
                "3. Implement fixes systematically",
                "4. Re-analyze to measure progress"
            ],
            "success_rate": "8/10 startups that fix critical issues get funded",
            "confidence": 0.9,
            "confidence_label": "Very High",
            "evidence_source": "WeReady Analysis",
            "priority": "critical",
            "market_context": "Critical threshold for investor consideration"
        }
    
    def _generate_improvement_roadmap_item(self, breakdown: 'ScoreBreakdown', timeline: str) -> Dict[str, Any]:
        """Generate improvement roadmap item for medium-scoring categories"""
        category_name = breakdown.category.value.replace('_', ' ').title()
        
        return {
            "action": f"Improve {category_name.lower()} to reach 75+ score",
            "title": f"Enhance {category_name} ({breakdown.score}/100)",
            "why": f"Pushing {category_name.lower()} above 75 puts you in top quartile for funding consideration.",
            "how": [
                f"1. Implement top {category_name.lower()} recommendations",
                "2. Follow industry best practices",
                "3. Measure improvement weekly",
                "4. Target 75+ score within 8 weeks"
            ],
            "success_rate": "6/10 startups improve 10+ points in 8 weeks",
            "confidence": 0.75,
            "confidence_label": "High",
            "evidence_source": "WeReady Success Data", 
            "priority": "high",
            "market_context": "Competitive advantage in funding process"
        }
    
    def _generate_strategic_roadmap_items(
        self, 
        overall_score: float, 
        key_risks: List[str] = None,
        competitive_moats: List[str] = None
    ) -> List[Dict[str, Any]]:
        """Generate strategic roadmap items based on overall performance"""
        
        items = []
        
        if overall_score >= 85:
            items.append({
                "action": "Apply for WeReady Stamp certification",
                "title": "Achieve WeReady Stamp Recognition",
                "why": "WeReady Stamp signals to investors that you've met rigorous startup readiness criteria.",
                "how": [
                    "1. Complete final WeReady assessment",
                    "2. Submit WeReady Stamp application",
                    "3. Use certification in fundraising materials",
                    "4. Join WeReady success community"
                ],
                "success_rate": "WeReady Stamp holders have 3x funding success rate",
                "confidence": 0.95,
                "confidence_label": "Very High",
                "evidence_source": "WeReady Success Metrics",
                "priority": "high",
                "market_context": "Differentiation in competitive funding market",
                "timeline": "short_term"
            })
        
        if key_risks:
            items.append({
                "action": "Mitigate identified key risks",
                "title": "Risk Management & Contingency Planning", 
                "why": "Proactive risk management demonstrates investor-ready thinking and reduces failure probability.",
                "how": [
                    "1. Create risk mitigation plans for each identified risk",
                    "2. Set up early warning indicators",
                    "3. Build contingency plans",
                    "4. Review and update monthly"
                ],
                "success_rate": "Startups with risk plans have 40% lower failure rates",
                "confidence": 0.8,
                "confidence_label": "High",
                "evidence_source": "Startup Risk Research",
                "priority": "medium",
                "market_context": "Due diligence preparation",
                "timeline": "long_term"
            })
        
        return items
    
    def _prioritize_roadmap_items(self, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Sort roadmap items by priority and confidence"""
        priority_weights = {"critical": 4, "high": 3, "medium": 2, "low": 1}
        
        return sorted(items, key=lambda x: (
            priority_weights.get(x.get("priority", "medium"), 2),
            x.get("confidence", 0.7)
        ), reverse=True)
    
    def generate_weready_report(self, score: WeReadyScore) -> Dict:
        """Generate a comprehensive WeReady report"""
        
        return {
            "overall_score": score.overall_score,
            "verdict": score.verdict,
            "weready_stamp_eligible": score.weready_stamp_eligible,
            "breakdown": {
                breakdown.category.value: {
                    "score": int(breakdown.score),
                    "status": breakdown.status,
                    "weight": int(breakdown.weight * 100),
                    "issues": breakdown.issues,
                    "recommendations": breakdown.recommendations,
                    "detailed_analysis": breakdown.detailed_analysis,
                    "insights": breakdown.insights,
                    "critical_issues": breakdown.critical_issues,
                    "quick_wins": breakdown.quick_wins,
                    "long_term_improvements": breakdown.long_term_improvements
                }
                for breakdown in score.breakdown
            },
            "next_steps": score.next_steps,
            "improvement_roadmap": score.improvement_roadmap,
            "market_context": {
                "percentile": self._calculate_percentile(score.overall_score),
                "comparison": self._generate_market_comparison(score.overall_score)
            }
        }
    
    def _calculate_percentile(self, score: int) -> str:
        """Calculate rough percentile based on score"""
        if score >= 90:
            return "Top 5%"
        elif score >= 80:
            return "Top 20%" 
        elif score >= 70:
            return "Top 40%"
        elif score >= 60:
            return "Top 60%"
        else:
            return "Bottom 40%"
    
    def _generate_market_comparison(self, score: int) -> str:
        """Generate market comparison text"""
        if score >= 85:
            return "Better than most YC startups at Demo Day"
        elif score >= 75:
            return "Above average for seed-stage startups"
        elif score >= 65:
            return "Typical early-stage startup score"
        elif score >= 50:
            return "Below average - needs significant work"
        else:
            return "Far below investment threshold"
    
    def _generate_code_quality_analysis(self, hallucination_result: Dict, repo_analysis: Dict, score: float) -> Dict:
        """Generate detailed code quality analysis breakdown"""
        
        analysis = {
            "ai_detection": {
                "likelihood": hallucination_result.get('ai_likelihood', 0),
                "confidence": hallucination_result.get('confidence', 0.8),
                "patterns_detected": [],
                "risk_level": "low"
            },
            "security_assessment": {
                "vulnerability_count": 0,
                "severity_distribution": {"critical": 0, "high": 0, "medium": 0, "low": 0},
                "security_score": 85
            },
            "architecture_quality": {
                "structure_score": 80,
                "maintainability": 75,
                "scalability": 70,
                "code_organization": "good"
            },
            "testing_coverage": {
                "estimated_coverage": 60,
                "test_quality": "moderate",
                "missing_tests": ["integration", "edge_cases"]
            }
        }
        
        # Enhance AI detection analysis
        ai_likelihood = hallucination_result.get('ai_likelihood', 0)
        if ai_likelihood > 0.8:
            analysis["ai_detection"]["risk_level"] = "high"
            analysis["ai_detection"]["patterns_detected"] = [
                "Repetitive code patterns",
                "Generic variable names",
                "Inconsistent coding style"
            ]
        elif ai_likelihood > 0.5:
            analysis["ai_detection"]["risk_level"] = "medium"
            analysis["ai_detection"]["patterns_detected"] = [
                "Some AI-generated patterns detected",
                "Mixed coding styles"
            ]
        
        # Hallucination impact on security
        hallucinated_packages = hallucination_result.get('hallucinated_packages', [])
        if hallucinated_packages:
            analysis["security_assessment"]["vulnerability_count"] = len(hallucinated_packages)
            analysis["security_assessment"]["severity_distribution"]["critical"] = len(hallucinated_packages)
            analysis["security_assessment"]["security_score"] = max(20, 85 - len(hallucinated_packages) * 15)
        
        return analysis
    
    def _generate_code_insights(self, hallucination_result: Dict, score: float) -> List[str]:
        """Generate actionable insights about code quality"""
        
        insights = []
        
        ai_likelihood = hallucination_result.get('ai_likelihood', 0)
        hallucination_score = hallucination_result.get('hallucination_score', 0)
        
        if score >= 90:
            insights.append("ğŸ† Excellent code quality - ready for production deployment")
            insights.append("ğŸ’¡ Consider implementing advanced monitoring and observability")
        elif score >= 75:
            insights.append("âœ… Good code foundation with room for optimization")
            insights.append("ğŸš€ Focus on performance tuning and scalability improvements")
        elif score >= 50:
            insights.append("âš ï¸ Code needs significant improvements before launch")
            insights.append("ğŸ”§ Priority: Address security vulnerabilities and testing gaps")
        else:
            insights.append("ğŸš¨ Critical code quality issues must be resolved")
            insights.append("â›” Not recommended for production without major refactoring")
        
        # AI-specific insights
        if ai_likelihood > 0.7:
            insights.append(f"ğŸ¤– High AI-generation detected ({ai_likelihood:.0%}) - implement comprehensive testing")
        
        if hallucination_score > 0:
            fake_count = len(hallucination_result.get('hallucinated_packages', []))
            insights.append(f"ğŸ” {fake_count} hallucinated packages found - immediate security risk")
        
        return insights
    
    def _categorize_code_improvements(self, issues: List[str], recommendations: List[str], score: float) -> tuple:
        """Categorize improvements by urgency and impact"""
        
        critical_issues = []
        quick_wins = []
        long_term = []
        
        # Categorize based on content and score
        for issue in issues:
            if "hallucinated" in issue.lower() or "security" in issue.lower():
                critical_issues.append(f"ğŸš¨ {issue}")
            elif "ai-generation" in issue.lower():
                critical_issues.append(f"âš ï¸ {issue}")
            else:
                quick_wins.append(f"ğŸ”§ {issue}")
        
        for rec in recommendations:
            if "security" in rec.lower() or "vulnerability" in rec.lower():
                critical_issues.append(f"ğŸ›¡ï¸ {rec}")
            elif "test" in rec.lower() or "coverage" in rec.lower():
                quick_wins.append(f"ğŸ§ª {rec}")
            elif "refactor" in rec.lower() or "architecture" in rec.lower():
                long_term.append(f"ğŸ—ï¸ {rec}")
            else:
                quick_wins.append(f"âœ… {rec}")
        
        # Add default improvements based on score
        if score < 80:
            if not any("test" in item.lower() for item in quick_wins):
                quick_wins.append("ğŸ§ª Implement comprehensive unit testing")
            if not any("monitoring" in item.lower() for item in long_term):
                long_term.append("ğŸ“Š Add application monitoring and logging")
        
        return critical_issues, quick_wins, long_term
    
    def _generate_business_model_analysis(self, business_data: Dict, score: float) -> Dict:
        """Generate detailed business model analysis breakdown"""
        
        analysis = {
            "revenue_model": {
                "clarity_score": 65,
                "revenue_streams": ["subscription", "usage-based"],
                "pricing_strategy": "freemium",
                "monetization_readiness": "early"
            },
            "market_validation": {
                "validation_score": 60,
                "customer_interviews": 15,
                "market_signals": ["positive feedback", "early adoption"],
                "validation_stage": "initial"
            },
            "unit_economics": {
                "economics_score": 55,
                "cac_ltv_ratio": "unknown",
                "gross_margins": "estimated 70%",
                "burn_rate": "sustainable"
            },
            "competitive_positioning": {
                "differentiation_score": 70,
                "unique_value_prop": "AI-first approach",
                "competitive_moats": ["proprietary data", "network effects"],
                "market_position": "emerging player"
            }
        }
        
        return analysis
    
    def _generate_business_insights(self, score: float) -> List[str]:
        """Generate actionable insights about business model"""
        
        insights = []
        
        if score >= 80:
            insights.append("ğŸš€ Strong business model foundation - ready for scaling")
            insights.append("ğŸ’° Focus on optimizing unit economics and expanding market reach")
        elif score >= 65:
            insights.append("ğŸ“ˆ Solid business fundamentals with room for improvement")
            insights.append("ğŸ¯ Priority: Strengthen customer validation and pricing strategy")
        elif score >= 50:
            insights.append("âš ï¸ Business model needs significant development")
            insights.append("ğŸ” Focus on product-market fit and revenue model clarity")
        else:
            insights.append("ğŸš¨ Critical business model issues need immediate attention")
            insights.append("â›” Not ready for investment without major business pivots")
        
        # Standard insights for AI startups
        insights.append("ğŸ¤– AI startups typically need 18+ months to achieve strong PMF")
        insights.append("ğŸ’¡ Consider freemium model to accelerate user acquisition")
        
        return insights
    
    def _categorize_business_improvements(self, issues: List[str], recommendations: List[str], score: float) -> tuple:
        """Categorize business improvements by urgency and impact"""
        
        critical_issues = []
        quick_wins = []
        long_term = []
        
        # Categorize based on content and score
        for rec in recommendations:
            if "revenue model" in rec.lower() or "pricing" in rec.lower():
                quick_wins.append(f"ğŸ’° {rec}")
            elif "market" in rec.lower() or "customer" in rec.lower():
                critical_issues.append(f"ğŸ¯ {rec}")
            elif "unit economics" in rec.lower() or "metrics" in rec.lower():
                long_term.append(f"ğŸ“Š {rec}")
            else:
                quick_wins.append(f"ğŸ“ˆ {rec}")
        
        # Add strategic improvements based on score
        if score < 70:
            quick_wins.append("ğŸ“ Create detailed customer personas and use cases")
            critical_issues.append("ğŸ—£ï¸ Conduct 25+ customer discovery interviews")
            long_term.append("ğŸ¢ Develop go-to-market strategy and sales process")
        
        return critical_issues, quick_wins, long_term
    
    def _generate_investment_analysis(self, investment_data: Dict, score: float) -> Dict:
        """Generate detailed investment readiness analysis breakdown"""
        
        analysis = {
            "traction_metrics": {
                "traction_score": 50,
                "revenue_growth": "early stage",
                "user_growth": "steady",
                "key_metrics": {"mrr": "TBD", "cac": "TBD", "ltv": "TBD"}
            },
            "team_assessment": {
                "team_score": 70,
                "team_completeness": 65,
                "domain_expertise": 75,
                "execution_track_record": "emerging"
            },
            "market_opportunity": {
                "market_score": 80,
                "addressable_market": "large",
                "market_timing": "excellent",
                "competitive_landscape": "fragmented"
            },
            "scalability_factors": {
                "scalability_score": 60,
                "technology_scalability": "high",
                "business_model_scalability": "moderate",
                "operational_scalability": "developing"
            }
        }
        
        return analysis
    
    def _generate_investment_insights(self, score: float) -> List[str]:
        """Generate actionable insights about investment readiness"""
        
        insights = []
        
        if score >= 75:
            insights.append("ğŸ† Strong investment readiness - Series A potential")
            insights.append("ğŸ“ˆ Focus on scaling traction and team building")
        elif score >= 60:
            insights.append("ğŸ“Š Good foundation for seed funding")
            insights.append("ğŸš€ Priority: Demonstrate strong traction and metrics")
        elif score >= 45:
            insights.append("âš ï¸ Pre-seed potential with significant development needed")
            insights.append("ğŸ”§ Focus on product-market fit and initial traction")
        else:
            insights.append("ğŸš¨ Not ready for institutional investment")
            insights.append("â›” Focus on bootstrapping and early validation")
        
        # AI-specific investment insights
        insights.append("ğŸ¤– AI startups typically require $500K+ to validate unit economics")
        insights.append("ğŸ’¡ Demonstrate defensible moats beyond AI implementation")
        
        return insights
    
    def _categorize_investment_improvements(self, issues: List[str], recommendations: List[str], score: float) -> tuple:
        """Categorize investment improvements by urgency and impact"""
        
        critical_issues = []
        quick_wins = []
        long_term = []
        
        # Categorize based on content and score  
        for rec in recommendations:
            if "metrics" in rec.lower() or "tracking" in rec.lower():
                critical_issues.append(f"ğŸ“Š {rec}")
            elif "product-market fit" in rec.lower() or "customers" in rec.lower():
                critical_issues.append(f"ğŸ¯ {rec}")
            elif "team" in rec.lower() or "hiring" in rec.lower():
                long_term.append(f"ğŸ‘¥ {rec}")
            elif "strategy" in rec.lower() or "market" in rec.lower():
                quick_wins.append(f"ğŸ“ˆ {rec}")
            else:
                quick_wins.append(f"ğŸ’¼ {rec}")
        
        # Add investment-specific improvements based on score
        if score < 65:
            critical_issues.append("ğŸ“ˆ Establish monthly recurring revenue tracking")
            quick_wins.append("ğŸ“ Create investor-ready financial projections")
            long_term.append("ğŸ¢ Build advisory board with industry experts")
        
        return critical_issues, quick_wins, long_term